{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JiiM6KzKxb7n"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZCUQzWYvy9cL"},"source":["%cd /gdrive/My Drive/2022_AN2DL(Private)/ExerciseSession4/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l3FoTyRa9pLu"},"source":["### Import libraries"]},{"cell_type":"code","metadata":{"id":"f_sOaV1Y8NsL"},"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import random\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.metrics import confusion_matrix\n","from PIL import Image\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pLb-N5JzUUQS"},"source":["### Set seed for reproducibility"]},{"cell_type":"code","metadata":{"id":"C7HYua8HUHIj"},"source":["# Random seed for reproducibility\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Suppress warnings"],"metadata":{"id":"BDM82PpE3VSg"}},{"cell_type":"code","source":["import warnings\n","import logging\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","tf.get_logger().setLevel('INFO')\n","tf.autograph.set_verbosity(0)\n","\n","tf.get_logger().setLevel(logging.ERROR)\n","tf.get_logger().setLevel('ERROR')\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"],"metadata":{"id":"5fXtacjAqOIq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MvjjNtBV_jBQ"},"source":["# UC Merced Land Use Dataset\n","\n","http://weegee.vision.ucmerced.edu/datasets/landuse.html\n","\n","Aerial RGB images of 256x256 pixels. Images belongs to 21 different classes representing the land uses.\n","\n","Class labels:\n","0. agricultural\n","1. airplane\n","2. baseballdiamond\n","3. beach\n","4 .buildings\n","5. chaparral\n","6. denseresidential\n","7. forest\n","8. freeway\n","9. golfcourse\n","10. harbor\n","11. intersection\n","12. mediumresidential\n","13. mobilehomepark\n","14. overpass\n","15. parkinglot\n","16. river\n","17. runway\n","18. sparseresidential\n","19. storagetanks\n","20. tenniscourt"]},{"cell_type":"code","metadata":{"id":"WRL3G8yL8BPL"},"source":["# Load the dataset to be used for classification\n","!unzip UCMerced_LandUse.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSmn5M8_PyJ1"},"source":["# Dataset folders \n","dataset_dir = 'UCMerced_LandUse'\n","training_dir = os.path.join(dataset_dir, 'training')\n","validation_dir = os.path.join(dataset_dir, 'validation')\n","test_dir = os.path.join(dataset_dir, 'test')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UrdOzQ2-Jqe"},"source":["# Plot example images from dataset\n","labels = ['agricultural',       # 0\n","          'airplane',           # 1\n","          'baseballdiamond',    # 2\n","          'beach',              # 3\n","          'buildings',          # 4\n","          'chaparral',          # 5\n","          'denseresidential',   # 6\n","          'forest',             # 7\n","          'freeway',            # 8\n","          'golfcourse',         # 9\n","          'harbor',             # 10\n","          'intersection',       # 11\n","          'mediumresidential',  # 12\n","          'mobilehomepark',     # 13\n","          'overpass',           # 14\n","          'parkinglot',         # 15\n","          'river',              # 16\n","          'runway',             # 17\n","          'sparseresidential',  # 18\n","          'storagetanks',       # 19\n","          'tenniscourt']        # 20\n","\n","num_row = 7\n","num_col = 3\n","fig, axes = plt.subplots(num_row, num_col, figsize=(2*num_row,9*num_col))\n","for i in range(num_row*num_col):\n","  if i < 21:\n","    class_imgs = next(os.walk('{}/training/{}/'.format(dataset_dir, labels[i])))[2]\n","    class_img = class_imgs[0]\n","    img = Image.open('{}/training/{}/{}'.format(dataset_dir, labels[i], class_img))\n","    ax = axes[i//num_col, i%num_col]\n","    ax.imshow(np.array(img))\n","    ax.set_title('{}'.format(labels[i]))\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-yzGkVx-5iZj"},"source":["# Data Loader\n","\n","### Typically, data is too large to be loaded as a NumPy array.\n","### We need a different strategy.."]},{"cell_type":"code","metadata":{"id":"bbP_y7gxE6pg"},"source":["print(\"Depth 0 (UCMerced_LandUse\")\n","print(\"-------\")\n","!ls UCMerced_LandUse"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Depth 1 (e.g., UCMerced_LandUse/training)\")\n","print(\"------------------------\")\n","!ls UCMerced_LandUse/training"],"metadata":{"id":"MnSQfB2g5Aha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Depth 2 (e.g., UCMerced_LandUse/training/agricultural)\")\n","print(\"----------------------------\")\n","!ls UCMerced_LandUse/training/agricultural"],"metadata":{"id":"xkt7dH265AeT"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DcWi9Iat96uk"},"source":["# Images are divided into folders, one for each class. \n","# If the images are organized in such a way, we can exploit the \n","# ImageDataGenerator to read them from disk.\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Create an instance of ImageDataGenerator for training, validation, and test sets\n","train_data_gen = ImageDataGenerator()\n","valid_data_gen = ImageDataGenerator()\n","test_data_gen = ImageDataGenerator()\n","\n","# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n","train_gen = train_data_gen.flow_from_directory(directory=training_dir,\n","                                               target_size=(256,256),\n","                                               color_mode='rgb',\n","                                               classes=None, # can be set to labels\n","                                               class_mode='categorical',\n","                                               batch_size=8,\n","                                               shuffle=True,\n","                                               seed=seed)\n","valid_gen = train_data_gen.flow_from_directory(directory=validation_dir,\n","                                               target_size=(256,256),\n","                                               color_mode='rgb',\n","                                               classes=None, # can be set to labels\n","                                               class_mode='categorical',\n","                                               batch_size=8,\n","                                               shuffle=False,\n","                                               seed=seed)\n","test_gen = train_data_gen.flow_from_directory(directory=test_dir,\n","                                              target_size=(256,256),\n","                                              color_mode='rgb',\n","                                              classes=None, # can be set to labels\n","                                              class_mode='categorical',\n","                                              batch_size=8,\n","                                              shuffle=False,\n","                                              seed=seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wothhodRGqqo"},"source":["print(\"Assigned labels\")\n","print(train_gen.class_indices)\n","print()\n","print(\"Target classes\")\n","print(train_gen.classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5D5ln0cHVL2b"},"source":["def get_next_batch(generator):\n","  batch = next(generator)\n","\n","  image = batch[0]\n","  target = batch[1]\n","\n","  print(\"(Input) image shape:\", image.shape)\n","  print(\"Target shape:\",target.shape)\n","\n","  # Visualize only the first sample\n","  image = image[0]\n","  target = target[0]\n","  target_idx = np.argmax(target)\n","  print()\n","  print(\"Categorical label:\", target)\n","  print(\"Label:\", target_idx)\n","  print(\"Class name:\", labels[target_idx])\n","  fig = plt.figure(figsize=(6, 4))\n","  plt.imshow(np.uint8(image))\n","\n","  return batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WJ6q1cJLalh7"},"source":["# Get a sample from dataset and show info\n","_ = get_next_batch(train_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UkKOgZcDcRf1"},"source":["# Data Augmentation\n","\n","##### ImageDataGenerator allows to perform data augmentation\n","\n","```\n","tf.keras.preprocessing.image.ImageDataGenerator(\n","    featurewise_center=False, samplewise_center=False,\n","    featurewise_std_normalization=False, samplewise_std_normalization=False,\n","    zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0,\n","    height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0,\n","    channel_shift_range=0.0, fill_mode='nearest', cval=0.0,\n","    horizontal_flip=False, vertical_flip=False, rescale=None,\n","    preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None\n",")\n","```"]},{"cell_type":"code","metadata":{"id":"-fUfDBugWlSS"},"source":["# Create some augmentation examples\n","# Get sample image\n","image = next(train_gen)[0][4]\n","\n","# Create an instance of ImageDataGenerator for each transformation\n","rot_gen = ImageDataGenerator(rotation_range=30)\n","shift_gen = ImageDataGenerator(width_shift_range=50)\n","zoom_gen = ImageDataGenerator(zoom_range=0.3)\n","flip_gen = ImageDataGenerator(horizontal_flip=True)\n","\n","# Get random transformations\n","rot_t = rot_gen.get_random_transform(img_shape=(256, 256), seed=seed)\n","print('Rotation:', rot_t, '\\n')\n","shift_t = shift_gen.get_random_transform(img_shape=(256, 256), seed=seed)\n","print('Shift:', shift_t, '\\n')\n","zoom_t = zoom_gen.get_random_transform(img_shape=(256, 256), seed=seed)\n","print('Zoom:', zoom_t, '\\n')\n","flip_t = flip_gen.get_random_transform(img_shape=(256, 256), seed=seed)\n","print('Flip:', flip_t, '\\n')\n","\n","# Apply the transformation\n","gen = ImageDataGenerator(fill_mode='constant', cval=0.)\n","rotated = gen.apply_transform(image, rot_t)\n","shifted = gen.apply_transform(image, shift_t) \n","zoomed = gen.apply_transform(image, zoom_t) \n","flipped = gen.apply_transform(image, flip_t)  \n","\n","# Plot original and augmented images\n","fig, ax = plt.subplots(1, 5, figsize=(15, 45))\n","ax[0].imshow(np.uint8(image))\n","ax[0].set_title('Original')\n","ax[1].imshow(np.uint8(rotated))\n","ax[1].set_title('Rotated')\n","ax[2].imshow(np.uint8(shifted))\n","ax[2].set_title('Shifted')\n","ax[3].imshow(np.uint8(zoomed))\n","ax[3].set_title('Zoomed')\n","ax[4].imshow(np.uint8(flipped))\n","ax[4].set_title('Flipped')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YbLbACrse4iB"},"source":["# Combine multiple transformations\n","gen = ImageDataGenerator(rotation_range=30,\n","                         height_shift_range=50,\n","                         width_shift_range=50,\n","                         zoom_range=0.3,\n","                         horizontal_flip=True,\n","                         vertical_flip=True, \n","                         fill_mode='reflect')\n","\n","# Get random transformation\n","t = gen.get_random_transform(img_shape=(256, 256), seed=seed)\n","print(\"Transform:\", t)\n","\n","# Apply the transformation\n","augmented = gen.apply_transform(image, t)\n","\n","# Plot original and augmented images\n","fig, ax = plt.subplots(1, 2, figsize=(15,30))\n","ax[0].imshow(np.uint8(image))\n","ax[0].set_title(\"Original\")\n","ax[1].imshow(np.uint8(augmented))\n","ax[1].set_title(\"Augmented\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K7eBGvBAocGL"},"source":["# Training with and without data augmentation"]},{"cell_type":"code","metadata":{"id":"QUA94y1ZoVFc"},"source":["# Create an instance of ImageDataGenerator with NO Data Augmentation\n","noaug_train_data_gen = ImageDataGenerator(rescale=1/255.) # rescale value is multiplied to the image\n","valid_data_gen = ImageDataGenerator(rescale=1/255.)\n","test_data_gen = ImageDataGenerator(rescale=1/255.)\n","\n","# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n","noaug_train_gen = noaug_train_data_gen.flow_from_directory(directory=training_dir,\n","                                                           target_size=(256,256),\n","                                                           color_mode='rgb',\n","                                                           classes=None, # can be set to labels\n","                                                           class_mode='categorical',\n","                                                           batch_size=8,\n","                                                           shuffle=True,\n","                                                           seed=seed)\n","valid_gen = valid_data_gen.flow_from_directory(directory=validation_dir,\n","                                               target_size=(256,256),\n","                                               color_mode='rgb',\n","                                               classes=None, # can be set to labels\n","                                               class_mode='categorical',\n","                                               batch_size=8,\n","                                               shuffle=False,\n","                                               seed=seed)\n","test_gen = test_data_gen.flow_from_directory(directory=test_dir,\n","                                             target_size=(256,256),\n","                                             color_mode='rgb',\n","                                             classes=None, # can be set to labels\n","                                             class_mode='categorical',\n","                                             batch_size=8,\n","                                             shuffle=False,\n","                                             seed=seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w62slIJEmbM8"},"source":["# Create an instance of ImageDataGenerator with Data Augmentation\n","aug_train_data_gen = ImageDataGenerator(rotation_range=30,\n","                                        height_shift_range=50,\n","                                        width_shift_range=50,\n","                                        zoom_range=0.3,\n","                                        horizontal_flip=True,\n","                                        vertical_flip=True, \n","                                        fill_mode='reflect',\n","                                        rescale=1/255.) # rescale value is multiplied to the image\n","\n","# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n","aug_train_gen = aug_train_data_gen.flow_from_directory(directory=training_dir,\n","                                                       target_size=(256,256),\n","                                                       color_mode='rgb',\n","                                                       classes=None, # can be set to labels\n","                                                       class_mode='categorical',\n","                                                       batch_size=8,\n","                                                       shuffle=True,\n","                                                       seed=seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jr9CX7CYdBg_"},"source":["### Models metadata"]},{"cell_type":"code","metadata":{"id":"7YcxBMJhl4EM"},"source":["input_shape = (256, 256, 3)\n","epochs = 200"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KXmw4F0wlY0h"},"source":["### CNN model"]},{"cell_type":"code","metadata":{"id":"PkccDbv-bzKr"},"source":["def build_model(input_shape):\n","\n","    # Build the neural network layer by layer\n","    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n","\n","    conv1 = tfkl.Conv2D(\n","        filters=32,\n","        kernel_size=3,\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.HeUniform(seed)\n","    )(input_layer)\n","    pool1 = tfkl.MaxPooling2D()(conv1)\n","\n","    conv2 = tfkl.Conv2D(\n","        filters=64,\n","        kernel_size=3,\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.HeUniform(seed)\n","    )(pool1)\n","    pool2 = tfkl.MaxPooling2D()(conv2)\n","\n","    conv3 = tfkl.Conv2D(\n","        filters=128,\n","        kernel_size=3,\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.HeUniform(seed)\n","    )(pool2)\n","    pool3 = tfkl.MaxPooling2D()(conv3)\n","\n","    conv4 = tfkl.Conv2D(\n","        filters=256,\n","        kernel_size=3,\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.HeUniform(seed)\n","    )(pool3)\n","    pool4 = tfkl.MaxPooling2D()(conv4)\n","\n","    conv5 = tfkl.Conv2D(\n","        filters=512,\n","        kernel_size=3,\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.HeUniform(seed)\n","    )(pool4)\n","    pool5 = tfkl.MaxPooling2D()(conv5)\n","\n","    flattening_layer = tfkl.Flatten(name='Flatten')(pool5)\n","    dropout = tfkl.Dropout(0.3, seed=seed)(flattening_layer)\n","    classifier_layer = tfkl.Dense(units=512, name='Classifier', kernel_initializer=tfk.initializers.HeUniform(seed), activation='relu')(dropout)\n","    dropout = tfkl.Dropout(0.3, seed=seed)(classifier_layer)\n","    output_layer = tfkl.Dense(units=21, activation='softmax', kernel_initializer=tfk.initializers.GlorotUniform(seed), name='output_layer')(dropout)\n","\n","    # Connect input and output through the Model class\n","    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n","\n","    # Compile the model\n","    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n","\n","    # Return the model\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qbixQiprQl_"},"source":["# Utility function to create folders and callbacks for training\n","from datetime import datetime\n","\n","def create_folders_and_callbacks(model_name):\n","\n","  exps_dir = os.path.join('data_augmentation_experiments')\n","  if not os.path.exists(exps_dir):\n","      os.makedirs(exps_dir)\n","\n","  now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n","  if not os.path.exists(exp_dir):\n","      os.makedirs(exp_dir)\n","      \n","  callbacks = []\n","\n","  # Model checkpoint\n","  # ----------------\n","  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","  if not os.path.exists(ckpt_dir):\n","      os.makedirs(ckpt_dir)\n","\n","  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), \n","                                                     save_weights_only=True, # True to save only weights\n","                                                     save_best_only=False) # True to save only the best epoch \n","  callbacks.append(ckpt_callback)\n","\n","  # Visualize Learning on Tensorboard\n","  # ---------------------------------\n","  tb_dir = os.path.join(exp_dir, 'tb_logs')\n","  if not os.path.exists(tb_dir):\n","      os.makedirs(tb_dir)\n","      \n","  # By default shows losses and metrics for both training and validation\n","  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n","                                               profile_batch=0,\n","                                               histogram_freq=1)  # if > 0 (epochs) shows weights histograms\n","  callbacks.append(tb_callback)\n","\n","  # Early Stopping\n","  # --------------\n","  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n","  callbacks.append(es_callback)\n","\n","  return callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwsjIp_crOKq"},"source":["# Build model (for NO augmentation training)\n","model = build_model(input_shape)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hln5Up0XtOs9"},"source":["# Create folders and callbacks and fit\n","noaug_callbacks = create_folders_and_callbacks(model_name='CNN_NoAug')\n","\n","# Train the model\n","history = model.fit(\n","    x = noaug_train_gen,\n","    epochs = epochs,\n","    validation_data = valid_gen,\n","    callbacks = noaug_callbacks,\n",").history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6m-yQ1bug3o"},"source":["# Save best epoch model\n","model.save(\"data_augmentation_experiments/CNN_NoAug_Best\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Wz8Y6AMzyyk"},"source":["# Build model (for data augmentation training)\n","model = build_model(input_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXAHAuoSz-_S"},"source":["# Create folders and callbacks and fit\n","aug_callbacks = create_folders_and_callbacks(model_name='CNN_Aug')\n","\n","# Train the model\n","history = model.fit(\n","    x = aug_train_gen,\n","    epochs = epochs,\n","    validation_data = valid_gen,\n","    callbacks = aug_callbacks,\n",").history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IwhA2qPP0Nvg"},"source":["# Save best epoch model\n","model.save(\"data_augmentation_experiments/CNN_Aug_Best\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86GXG1T-02-t"},"source":["# Evaluate on test\n","# Trainined with no data augmentation\n","model_noaug = tfk.models.load_model(\"data_augmentation_experiments/CNN_NoAug_Best\")\n","model_noaug_test_metrics = model_noaug.evaluate(test_gen, return_dict=True)\n","# Trained with data augmentation\n","model_aug = tfk.models.load_model(\"data_augmentation_experiments/CNN_Aug_Best\")\n","model_aug_test_metrics = model_aug.evaluate(test_gen, return_dict=True)\n","\n","print()\n","print(\"Test metrics without data augmentation\")\n","print(model_noaug_test_metrics)\n","print(\"Test metrics with data augmentation\")\n","print(model_aug_test_metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EPTEvR1UQ9Q_"},"source":["# Visualize the activations"]},{"cell_type":"code","metadata":{"id":"aBJRVo7wjrvr"},"source":["# Get sample batch\n","batch = next(test_gen)[0]\n","\n","# Get first image\n","image = batch[3] # batch size = 8\n","\n","fig = plt.figure(figsize=(6, 4))\n","plt.imshow(np.uint8(image*255))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CMmb-cPjrv2"},"source":["# Get the activations (the output of each ReLU layer)\n","# We can do it by creating a new Model (activation_model) with the same input as \n","# the original model and all the ReLU activations as output\n","layers = [layer.output for layer in model_aug.layers if isinstance(layer, tf.keras.layers.Conv2D)]\n","activation_model = tf.keras.Model(inputs=model_aug.input, outputs=layers)\n","# Finally we get the output feature maps (for each layer) given the imput test image\n","fmaps = activation_model.predict(tf.expand_dims(image, 0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2iJcicAjrv2"},"source":["import matplotlib.pyplot as plt\n","from mpl_toolkits.axes_grid1 import ImageGrid\n","%matplotlib inline\n","def display_activation(fmaps, depth=0, first_n=-1): \n","    # fmaps: list of all the feature maps for each layer\n","    # depth: the layer we want to visualize (an int in [0, network depth))\n","    # first_n: default '-1' means 'all activations'. Number of activations to be visualized. Note that for deep layers it could be a large number.\n","\n","    fmaps = fmaps[depth] # get feature maps at the desired depth\n","    if first_n > 0:\n","      fmaps = fmaps[0, :, :, :first_n] \n","    fmaps = tf.image.resize(fmaps, size=[128, 128]) # resize for visualization\n","\n","    # Distribute on a grid for plotting\n","    col_size = 8\n","    row_size = fmaps.shape[-1] // 8\n","    fmap_channel=0\n","    fig = plt.figure(figsize=(30, 30))\n","    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n","                    nrows_ncols=(row_size, col_size),  \n","                    axes_pad=0.1,  # pad between axes in inch.\n","                    )\n","    for row in range(0,row_size):\n","        for col in range(0,col_size):\n","            grid[fmap_channel].imshow(fmaps[0, :, :, fmap_channel], cmap='gray', aspect='auto')\n","            fmap_channel += 1\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FlfBw24_5_D"},"source":["display_activation(fmaps=fmaps, depth=2, first_n=-1)"],"execution_count":null,"outputs":[]}]}